{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering notebook\n",
    "\n",
    "This is the final version of feature engineering notebook for solar model where we prepare our data for prediction. Prediction will be placed in a separate notebook. Going a little-bit upfront, we will be trying several regression models with our data set, including Random Forest algorithm and perhaps Neural Networks. Due to decision trees nature, the features will have to be engineered slightly differently:\n",
    "> Example: \n",
    "> * Random forests do not benefit from too large feature spaces, since they select at random sqrt(p) or p/3 features, where p is total number of features. Therefore, when working with discretization and encoding of some of the numerical variables it is beneficial not to increase the feature space by one-hot encoding and using dummy variables. Instead, we will keep them in the same feature as a categorical variable.\n",
    "> * For Neural Networks we would perform data scaling that is unneccessary for decision tree algorithms.\n",
    "\n",
    "### Important notice: variable division transformations are done on the Train set\n",
    "\n",
    "Checklist and a quick overview of feature engineering steps:\n",
    "\n",
    "* missing values\n",
    "* outliers\n",
    "* discretization\n",
    "* gaussian transformation (?)\n",
    "* new features\n",
    "* feature selection (+FMI API recieved data sneakpeak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data  \n",
    "Notes:    \n",
    "- Work on copies of the data (keep the original dataset intact).  \n",
    "- Write functions for all data transformations you apply, for five reasons:  \n",
    "    - So you can easily prepare the data the next time you get a fresh dataset  \n",
    "    - So you can apply these transformations in future projects  \n",
    "    - To clean and prepare the test set  \n",
    "    - To clean and prepare new data instances  \n",
    "    - To make it easy to treat your preparation choices as hyperparameters  \n",
    "\n",
    "1. Data cleaning:  \n",
    "    - Fix or remove outliers (optional).  \n",
    "    - Fill in missing values (e.g., with zero, mean, median...) or drop their rows (or columns).  \n",
    "2. Feature selection (optional):  \n",
    "    - Drop the attributes that provide no useful information for the task.  \n",
    "3. Feature engineering, where appropriates:  \n",
    "    - Discretize continuous features.  \n",
    "    - Decompose features (e.g., categorical, date/time, etc.).  \n",
    "    - Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\n",
    "    - Aggregate features into promising new features.  \n",
    "4. Feature scaling: standardize or normalize features.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove snow_depth, horizontal visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
